{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "lora_r = 8\n",
    "lora_alpha = 8\n",
    "lora_dropout = 0.2\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = False\n",
    "output_dir = \"./results\"\n",
    "fp16 = True\n",
    "bf16 = False\n",
    "num_train_epochs = 1\n",
    "per_device_training_batch = 1\n",
    "per_device_eval_batch = 1\n",
    "gradient_accumilation = 1\n",
    "gradient_checkpointing = True\n",
    "max_grad_norm = 0.3\n",
    "learning_rate = 2e-4\n",
    "weight_decay = 0.03\n",
    "optim = \"paged_adamw_8bit\"\n",
    "lr_scheduler = \"cosine\"\n",
    "warmup_ratio = 0.01\n",
    "group_by_length = True\n",
    "sav_steps = 0\n",
    "logging_steps = 25\n",
    "max_sequene_length = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretained(model_name,add_eos_token=True,add_bos_token=True,trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")[\"train\"]\n",
    "eval_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")[\"eval\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch,bnb_4bit_compute_dtype)\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=use_4bit,\n",
    "                                bnb_4bit_compute_dtype = compute_dtype,\n",
    "                                bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
    "                                bnb_4bit_use_double_quant = use_nested_quant)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name=model_name,\n",
    "                                             quantization_config = bnb_config,\n",
    "                                             device_map=\"auto\") # will split the model across the GPUs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(lora_alpha=lora_alpha,\n",
    "                         lora_dropout = lora_dropout,\n",
    "                         r = lora_r,\n",
    "                         bias = \"none\",\n",
    "                         task_type = \"CAUSAL_LM\")\n",
    "model = get_peft_model(model, peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    logits = torch.tensor(logits).reshape(-1,logits.shape[-1])\n",
    "    labels = torch.tensor(labels).reshape(-1)\n",
    "    mask = labels != 100\n",
    "\n",
    "    masked_logits = logits[mask]\n",
    "    masked_labels = labels[mask]\n",
    "\n",
    "    predictions = torch.argmax(masked_logits, dim=-1)\n",
    "    accuracy_metric = evaluate,load(\"accuracy\")\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, refrences = masked_labels)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = SFTConfig(\n",
    "    output_dir = output_dir,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    gradient_accumilation_steps = gradient_accumilation,\n",
    "    optim = optim,\n",
    "    save_steps = sav_steps,\n",
    "    logging_steps = logging_steps,\n",
    "    weight_decay = weight_decay,\n",
    "    fp16 = fp16,\n",
    "    bf16 = bf16,\n",
    "    per_device_training_batch = per_device_training_batch,\n",
    "    eval_strategy = \"epoch\",\n",
    "    eval_accumulation_steps = 1,\n",
    "    max_grad_norm = max_grad_norm,\n",
    "    group_by_length = group_by_length,\n",
    "    lr_scheduler_type = lr_scheduler\n",
    ")\n",
    "\n",
    "trainer = SFTrainer(\n",
    "    model=model,\n",
    "    args = training_arguments,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    tokenizer = tokenizer,\n",
    "    max_sequene_length = max_sequene_length\n",
    "    compute_metrics = compute_metrics\n",
    ")\n",
    "\n",
    "result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
